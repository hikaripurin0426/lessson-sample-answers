#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã€åˆå¿ƒè€…å‘ã‘ã€‘ãƒªã‚¹ãƒˆè¦ç´ ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç·´ç¿’
ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰æ§˜ã€…ãªãƒªã‚¹ãƒˆè¦ç´ ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ:
- ul (é †åºãªã—ãƒªã‚¹ãƒˆ) ã®å–å¾—
- ol (é †åºã‚ã‚Šãƒªã‚¹ãƒˆ) ã®å–å¾—
- li (ãƒªã‚¹ãƒˆé …ç›®) ã®å–å¾—
- ãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆã®å‡¦ç†
"""

import requests
from bs4 import BeautifulSoup

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL
URL = "https://scraping-practice-six.vercel.app/list"

def get_soup():
    """Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    print("ğŸ“¡ Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    response = requests.get(URL, headers=headers)
    response.raise_for_status()
    print("âœ“ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ")
    return BeautifulSoup(response.content, 'html.parser')

def scrape_unordered_lists(soup):
    """é †åºãªã—ãƒªã‚¹ãƒˆ (ul) ã‚’å–å¾—"""
    print("1. é †åºãªã—ãƒªã‚¹ãƒˆ (ul) ã®å–å¾—")
    print("-" * 30)
    
    ul_lists = soup.find_all('ul')
    print(f"ulè¦ç´ æ•°: {len(ul_lists)}å€‹")
    
    for i, ul in enumerate(ul_lists, 1):
        ul_id = ul.get('id', 'ãªã—')
        ul_class = ' '.join(ul.get('class', []))
        
        print(f"\nãƒªã‚¹ãƒˆ{i}:")
        print(f"  id: {ul_id}")
        print(f"  class: {ul_class if ul_class else 'ãªã—'}")
        
        # ç›´ä¸‹ã®liè¦ç´ ã®ã¿ã‚’å–å¾—ï¼ˆãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆã®é …ç›®ã¯é™¤å¤–ï¼‰
        direct_items = ul.find_all('li', recursive=False)
        print(f"  é …ç›®æ•°: {len(direct_items)}å€‹")
        
        for j, li in enumerate(direct_items, 1):
            text = li.get_text(strip=True)
            # ãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯æœ€åˆã®æ–‡ã ã‘å–å¾—
            if li.find(['ul', 'ol']):
                # å­è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–ã—ã¦ã€ç›´æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å–å¾—
                direct_text = ''.join(li.find_all(text=True, recursive=False)).strip()
                text = direct_text if direct_text else text.split('\n')[0]
            
            print(f"    {j}. {text}")
    print()

def scrape_ordered_lists(soup):
    """é †åºã‚ã‚Šãƒªã‚¹ãƒˆ (ol) ã‚’å–å¾—"""
    print("2. é †åºã‚ã‚Šãƒªã‚¹ãƒˆ (ol) ã®å–å¾—")
    print("-" * 30)
    
    ol_lists = soup.find_all('ol')
    print(f"olè¦ç´ æ•°: {len(ol_lists)}å€‹")
    
    for i, ol in enumerate(ol_lists, 1):
        ol_id = ol.get('id', 'ãªã—')
        ol_class = ' '.join(ol.get('class', []))
        start = ol.get('start', '1')
        list_type = ol.get('type', 'ãªã—')
        
        print(f"\nãƒªã‚¹ãƒˆ{i}:")
        print(f"  id: {ol_id}")
        print(f"  class: {ol_class if ol_class else 'ãªã—'}")
        print(f"  start: {start}")
        print(f"  type: {list_type}")
        
        # ç›´ä¸‹ã®liè¦ç´ ã®ã¿ã‚’å–å¾—
        direct_items = ol.find_all('li', recursive=False)
        print(f"  é …ç›®æ•°: {len(direct_items)}å€‹")
        
        for j, li in enumerate(direct_items, 1):
            text = li.get_text(strip=True)
            # ãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯æœ€åˆã®æ–‡ã ã‘å–å¾—
            if li.find(['ul', 'ol']):
                direct_text = ''.join(li.find_all(text=True, recursive=False)).strip()
                text = direct_text if direct_text else text.split('\n')[0]
            
            print(f"    {j}. {text}")
    print()

def scrape_nested_lists(soup):
    """ãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆã‚’è©³ç´°ã«å–å¾—"""
    print("3. ãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆã®è©³ç´°å–å¾—")
    print("-" * 30)
    
    def process_list(list_element, level=0):
        """ãƒªã‚¹ãƒˆã‚’å†å¸°çš„ã«å‡¦ç†ã™ã‚‹é–¢æ•°"""
        indent = "  " * level
        list_type = list_element.name  # 'ul' ã¾ãŸã¯ 'ol'
        
        items = list_element.find_all('li', recursive=False)
        print(f"{indent}{list_type}ãƒªã‚¹ãƒˆ ({len(items)}é …ç›®):")
        
        for i, li in enumerate(items, 1):
            # ç›´æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            direct_text = ''.join(li.find_all(text=True, recursive=False)).strip()
            print(f"{indent}  {i}. {direct_text}")
            
            # å­ãƒªã‚¹ãƒˆãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            child_lists = li.find_all(['ul', 'ol'], recursive=False)
            for child_list in child_lists:
                process_list(child_list, level + 2)
    
    # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’æ¤œç´¢
    top_lists = soup.find_all(['ul', 'ol'])
    nested_lists = []
    
    for lst in top_lists:
        # å­ãƒªã‚¹ãƒˆã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯
        if lst.find(['ul', 'ol']):
            nested_lists.append(lst)
    
    print(f"ãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆ: {len(nested_lists)}å€‹")
    
    for i, lst in enumerate(nested_lists, 1):
        print(f"\nãƒã‚¹ãƒˆãƒªã‚¹ãƒˆ{i}:")
        process_list(lst)
    print()

def scrape_definition_lists(soup):
    """å®šç¾©ãƒªã‚¹ãƒˆ (dl, dt, dd) ã‚’å–å¾—"""
    print("4. å®šç¾©ãƒªã‚¹ãƒˆ (dl, dt, dd) ã®å–å¾—")
    print("-" * 30)
    
    dl_lists = soup.find_all('dl')
    print(f"dlè¦ç´ æ•°: {len(dl_lists)}å€‹")
    
    for i, dl in enumerate(dl_lists, 1):
        dl_id = dl.get('id', 'ãªã—')
        dl_class = ' '.join(dl.get('class', []))
        
        print(f"\nå®šç¾©ãƒªã‚¹ãƒˆ{i}:")
        print(f"  id: {dl_id}")
        print(f"  class: {dl_class if dl_class else 'ãªã—'}")
        
        # dt (å®šç¾©èª) ã¨ dd (å®šç¾©å†…å®¹) ã‚’å–å¾—
        dts = dl.find_all('dt')
        dds = dl.find_all('dd')
        
        print(f"  å®šç¾©èªæ•°: {len(dts)}å€‹")
        print(f"  å®šç¾©å†…å®¹æ•°: {len(dds)}å€‹")
        
        # dt ã¨ dd ã‚’ãƒšã‚¢ã§è¡¨ç¤º
        for j, (dt, dd) in enumerate(zip(dts, dds), 1):
            dt_text = dt.get_text(strip=True)
            dd_text = dd.get_text(strip=True)
            
            print(f"    {j}. {dt_text}: {dd_text}")
    print()



def scrape_lists_by_class(soup):
    """ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    print("6. ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ãƒªã‚¹ãƒˆã®å–å¾—")
    print("-" * 30)
    
    # ã‚¯ãƒ©ã‚¹åã§ãƒªã‚¹ãƒˆã‚’æ¤œç´¢
    classes = ['programming-languages', 'tech-categories']
    
    for class_name in classes:
        # å…¨ã¦ã®ul, olè¦ç´ ã‚’å–å¾—
        all_lists = soup.find_all(['ul', 'ol'])
        elements = []
        
        # å„ãƒªã‚¹ãƒˆè¦ç´ ã®ã‚¯ãƒ©ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        for lst in all_lists:
            class_attr = lst.get('class')
            if class_attr:  # ã‚¯ãƒ©ã‚¹å±æ€§ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                class_string = ' '.join(class_attr)  # ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                if class_name in class_string:  # æŒ‡å®šã—ãŸã‚¯ãƒ©ã‚¹åãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    elements.append(lst)
        
        if elements:
            print(f"\n'{class_name}'ã‚’å«ã‚€ã‚¯ãƒ©ã‚¹ã®ãƒªã‚¹ãƒˆ: {len(elements)}å€‹")
            for i, element in enumerate(elements, 1):
                element_class = ' '.join(element.get('class', []))
                items = element.find_all('li', recursive=False)
                
                print(f"  {i}. class='{element_class}' ({len(items)}é …ç›®)")
                for j, item in enumerate(items[:3], 1):  # æœ€åˆã®3é …ç›®ã®ã¿è¡¨ç¤º
                    text = item.get_text(strip=True)[:30]
                    print(f"     {j}. {text}...")
    print()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ãƒ¡ãƒ‹ãƒ¥ãƒ¼å½¢å¼ã§å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠ"""
    menu = [
        ("é †åºãªã—ãƒªã‚¹ãƒˆ (ul) ã®å–å¾—", scrape_unordered_lists),
        ("é †åºã‚ã‚Šãƒªã‚¹ãƒˆ (ol) ã®å–å¾—", scrape_ordered_lists),
        ("ãƒã‚¹ãƒˆã—ãŸãƒªã‚¹ãƒˆã®è©³ç´°å–å¾—", scrape_nested_lists),
        ("å®šç¾©ãƒªã‚¹ãƒˆ (dl, dt, dd) ã®å–å¾—", scrape_definition_lists),
        ("ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ãƒªã‚¹ãƒˆã®å–å¾—", scrape_lists_by_class),
    ]
    
    print("ğŸ“‹ ãƒªã‚¹ãƒˆè¦ç´ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç·´ç¿’")
    print("å¯¾è±¡: ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ (/list)")
    print("=" * 50)
    
    for i, (name, _) in enumerate(menu, 1):
        print(f"{i}. {name}")
    print("0. çµ‚äº†")
    print("=" * 50)
    
    # ä¸€åº¦ã ã‘Webãƒšãƒ¼ã‚¸ã‚’å–å¾—
    soup = get_soup()
    
    while True:
        try:
            choice = int(input("\nå®Ÿè¡Œã—ãŸã„ç•ªå·ã‚’é¸ã‚“ã§ãã ã•ã„: "))
        except ValueError:
            print("âŒ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            continue
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue
        
        if choice == 0:
            print("ğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        elif 1 <= choice <= len(menu):
            print("\n" + "=" * 50)
            menu[choice - 1][1](soup)
        else:
            print("âŒ æœ‰åŠ¹ãªç•ªå·ã‚’é¸ã‚“ã§ãã ã•ã„")

if __name__ == "__main__":
    main()
