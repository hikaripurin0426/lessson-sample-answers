#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã€åˆå¿ƒè€…å‘ã‘ã€‘HTMLå±æ€§ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç·´ç¿’
ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å±æ€§ãƒšãƒ¼ã‚¸ã‹ã‚‰æ§˜ã€…ãªHTMLå±æ€§ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ:
- get()ãƒ¡ã‚½ãƒƒãƒ‰ã§å±æ€§å€¤ã‚’å–å¾—
- dataå±æ€§ã®æŠ½å‡º
- è¤‡æ•°ã®å±æ€§ã‚’æŒã¤è¦ç´ ã®å‡¦ç†
"""

import requests
from bs4 import BeautifulSoup

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL
URL = "https://scraping-practice-six.vercel.app/attributes"

def get_soup():
    """Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    print("ğŸ“¡ Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    response = requests.get(URL, headers=headers)
    response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ
    print("âœ“ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ")
    return BeautifulSoup(response.content, 'html.parser')

def scrape_basic_attributes(soup):
    """åŸºæœ¬çš„ãªHTMLå±æ€§ã‚’æŠ½å‡ºã™ã‚‹"""
    print("1. åŸºæœ¬çš„ãªHTMLå±æ€§")
    print("-" * 30)
    
    # idå±æ€§ã‚’æŒã¤è¦ç´ ã‚’æ¤œç´¢
    elements_with_id = soup.find_all(attrs={'id': True})
    print(f"idå±æ€§ã‚’æŒã¤è¦ç´ : {len(elements_with_id)}å€‹")
    for element in elements_with_id[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
        print(f"  - {element.name}ã‚¿ã‚°: id='{element.get('id')}'")
    
    # classå±æ€§ã‚’æŒã¤è¦ç´ ã‚’æ¤œç´¢
    elements_with_class = soup.find_all(attrs={'class': True})
    print(f"\nclasså±æ€§ã‚’æŒã¤è¦ç´ : {len(elements_with_class)}å€‹")
    for element in elements_with_class[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
        classes = element.get('class', [])
        print(f"  - {element.name}ã‚¿ã‚°: class='{' '.join(classes)}'")
    print()

def scrape_link_attributes(soup):
    """ãƒªãƒ³ã‚¯è¦ç´ ã®å±æ€§ã‚’è©³ã—ãæŠ½å‡ºã™ã‚‹"""
    print("2. ãƒªãƒ³ã‚¯è¦ç´ ã®å±æ€§")
    print("-" * 30)
    
    links = soup.find_all('a')
    for i, link in enumerate(links, 1):
        href = link.get('href', 'ãªã—')
        target = link.get('target', 'ãªã—')
        title = link.get('title', 'ãªã—')
        text = link.get_text(strip=True)
        
        print(f"ãƒªãƒ³ã‚¯{i}:")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        print(f"  href: {href}")
        print(f"  target: {target}")
        print(f"  title: {title}")
        print()

def scrape_image_attributes(soup):
    """ç”»åƒè¦ç´ ã®å±æ€§ã‚’è©³ã—ãæŠ½å‡ºã™ã‚‹"""
    print("3. ç”»åƒè¦ç´ ã®å±æ€§")
    print("-" * 30)
    
    images = soup.find_all('img')
    for i, img in enumerate(images, 1):
        src = img.get('src', 'ãªã—')
        alt = img.get('alt', 'ãªã—')
        width = img.get('width', 'ãªã—')
        height = img.get('height', 'ãªã—')
        class_names = img.get('class', [])
        
        print(f"ç”»åƒ{i}:")
        print(f"  src: {src}")
        print(f"  alt: {alt}")
        print(f"  width: {width}")
        print(f"  height: {height}")
        print(f"  class: {' '.join(class_names) if class_names else 'ãªã—'}")
        print()

def scrape_data_attributes(soup):
    """dataå±æ€§ã‚’æŠ½å‡ºã™ã‚‹"""
    print("4. dataå±æ€§ã®æŠ½å‡º")
    print("-" * 30)
    
    # dataå±æ€§ã‚’æŒã¤è¦ç´ ã‚’æ¤œç´¢
    all_elements = soup.find_all()
    data_elements = []
    
    for element in all_elements:
        # å…¨ã¦ã®å±æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦data-ã§å§‹ã¾ã‚‹ã‚‚ã®ã‚’æ¢ã™
        data_attrs = {k: v for k, v in element.attrs.items() if k.startswith('data-')}
        if data_attrs:
            data_elements.append((element, data_attrs))
    
    print(f"dataå±æ€§ã‚’æŒã¤è¦ç´ : {len(data_elements)}å€‹")
    for element, data_attrs in data_elements[:10]:  # æœ€åˆã®10å€‹ã ã‘è¡¨ç¤º
        print(f"  {element.name}ã‚¿ã‚°:")
        for attr_name, attr_value in data_attrs.items():
            print(f"    {attr_name}: {attr_value}")
        print()

def scrape_form_attributes(soup):
    """ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®å±æ€§ã‚’æŠ½å‡ºã™ã‚‹"""
    print("5. ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®å±æ€§")
    print("-" * 30)
    
    # inputè¦ç´ ã®å±æ€§ã‚’å–å¾—
    inputs = soup.find_all('input')
    for i, input_elem in enumerate(inputs, 1):
        input_type = input_elem.get('type', 'ãªã—')
        name = input_elem.get('name', 'ãªã—')
        placeholder = input_elem.get('placeholder', 'ãªã—')
        required = input_elem.get('required', 'ãªã—')
        
        print(f"å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰{i}:")
        print(f"  type: {input_type}")
        print(f"  name: {name}")
        print(f"  placeholder: {placeholder}")
        print(f"  required: {required}")
        print()

def scrape_style_attributes(soup):
    """styleå±æ€§ã¨ãã®ä»–ã®è£…é£¾å±æ€§ã‚’æŠ½å‡ºã™ã‚‹"""
    print("6. styleå±æ€§ã¨ãã®ä»–ã®è£…é£¾å±æ€§")
    print("-" * 30)
    
    # styleå±æ€§ã‚’æŒã¤è¦ç´ ã‚’æ¤œç´¢
    styled_elements = soup.find_all(attrs={'style': True})
    print(f"styleå±æ€§ã‚’æŒã¤è¦ç´ : {len(styled_elements)}å€‹")
    
    for i, element in enumerate(styled_elements[:5], 1):
        style = element.get('style', '')
        print(f"è¦ç´ {i} ({element.name}ã‚¿ã‚°):")
        print(f"  style: {style}")
        print()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ãƒ¡ãƒ‹ãƒ¥ãƒ¼å½¢å¼ã§å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠ"""
    menu = [
        ("åŸºæœ¬çš„ãªHTMLå±æ€§", scrape_basic_attributes),
        ("ãƒªãƒ³ã‚¯è¦ç´ ã®å±æ€§", scrape_link_attributes),
        ("ç”»åƒè¦ç´ ã®å±æ€§", scrape_image_attributes),
        ("dataå±æ€§ã®æŠ½å‡º", scrape_data_attributes),
        ("ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®å±æ€§", scrape_form_attributes),
        ("styleå±æ€§ã¨ãã®ä»–ã®è£…é£¾å±æ€§", scrape_style_attributes),
    ]
    
    print("ğŸ” HTMLå±æ€§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç·´ç¿’")
    print("å¯¾è±¡: å±æ€§ãƒšãƒ¼ã‚¸ (/attributes)")
    print("=" * 50)
    
    for i, (name, _) in enumerate(menu, 1):
        print(f"{i}. {name}")
    print("0. çµ‚äº†")
    print("=" * 50)
    
    # ä¸€åº¦ã ã‘Webãƒšãƒ¼ã‚¸ã‚’å–å¾—
    soup = get_soup()
    
    while True:
        try:
            choice = int(input("\nå®Ÿè¡Œã—ãŸã„ç•ªå·ã‚’é¸ã‚“ã§ãã ã•ã„: "))
        except ValueError:
            print("âŒ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            continue
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue
        
        if choice == 0:
            print("ğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        elif 1 <= choice <= len(menu):
            print("\n" + "=" * 50)
            menu[choice - 1][1](soup)
        else:
            print("âŒ æœ‰åŠ¹ãªç•ªå·ã‚’é¸ã‚“ã§ãã ã•ã„")

if __name__ == "__main__":
    main()
