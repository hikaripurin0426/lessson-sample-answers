#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç·´ç¿’
ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ•ã‚©ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‹ã‚‰æ§˜ã€…ãªãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ:
- inputè¦ç´ ã®ç¨®é¡åˆ¥å–å¾—
- selectè¦ç´ ã¨optionè¦ç´ ã®å–å¾—
"""

import requests
from bs4 import BeautifulSoup

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL
URL = "https://scraping-practice-six.vercel.app/form"

def get_soup():
    """Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    print("ğŸ“¡ Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    response = requests.get(URL, headers=headers)
    response.raise_for_status()
    print("âœ“ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ")
    return BeautifulSoup(response.content, 'html.parser')

def scrape_form_basic_info(soup):
    """ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—"""
    print("1. ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®åŸºæœ¬æƒ…å ±")
    print("-" * 30)
    
    forms = soup.find_all('form')
    print(f"ãƒšãƒ¼ã‚¸å†…ã®ãƒ•ã‚©ãƒ¼ãƒ æ•°: {len(forms)}å€‹")
    
    for i, form in enumerate(forms, 1):
        form_id = form.get('id', 'ãªã—')
        
        print(f"\nãƒ•ã‚©ãƒ¼ãƒ {i}:")
        print(f"  id: {form_id}")
    print()

def scrape_input_elements(soup):
    """inputè¦ç´ ã‚’ç¨®é¡åˆ¥ã«åˆ†é¡ã—ã¦å–å¾—"""
    print("2. inputè¦ç´ ã®è©³ç´°æƒ…å ±")
    print("-" * 30)
    
    inputs = soup.find_all('input')
    print(f"inputè¦ç´ ã®ç·æ•°: {len(inputs)}å€‹")
    
    # inputè¦ç´ ã‚’ç¨®é¡åˆ¥ã«åˆ†é¡
    input_types = {}
    for input_elem in inputs:
        input_type = input_elem.get('type', 'text')
        if input_type not in input_types:
            input_types[input_type] = []
        input_types[input_type].append(input_elem)
    
    # ç¨®é¡åˆ¥ã«è¡¨ç¤º
    for input_type, elements in input_types.items():
        print(f"\n{input_type}ã‚¿ã‚¤ãƒ—: {len(elements)}å€‹")
        for i, elem in enumerate(elements, 1):
            name = elem.get('name', 'ãªã—')
            placeholder = elem.get('placeholder', 'ãªã—')
            required = 'ã‚ã‚Š' if elem.get('required') else 'ãªã—'
            
            print(f"  {i}. name: {name}")
            print(f"     placeholder: {placeholder}")
            print(f"     required: {required}")
    print()

def scrape_select_elements(soup):
    """selectè¦ç´ ã¨optionè¦ç´ ã‚’å–å¾—"""
    print("3. selectè¦ç´ ã¨optionè¦ç´ ")
    print("-" * 30)
    
    selects = soup.find_all('select')
    print(f"selectè¦ç´ æ•°: {len(selects)}å€‹")
    
    for i, select in enumerate(selects, 1):
        name = select.get('name', 'ãªã—')
        select_id = select.get('id', 'ãªã—')
        multiple = 'ã‚ã‚Š' if select.get('multiple') else 'ãªã—'
        
        print(f"\nã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹{i}:")
        print(f"  name: {name}")
        print(f"  id: {select_id}")
        print(f"  multiple: {multiple}")
        
        # optionè¦ç´ ã‚’å–å¾—
        options = select.find_all('option')
        print(f"  optionæ•°: {len(options)}å€‹")
        for j, option in enumerate(options, 1):
            value = option.get('value', 'ãªã—')
            text = option.get_text(strip=True)
            selected = 'ã‚ã‚Š' if option.get('selected') else 'ãªã—'
            
            print(f"    {j}. value: {value}, text: {text}, selected: {selected}")
    print()

def scrape_textarea_elements(soup):
    """textareaè¦ç´ ã‚’å–å¾—"""
    print("4. textareaè¦ç´ ")
    print("-" * 30)
    
    textareas = soup.find_all('textarea')
    print(f"textareaè¦ç´ æ•°: {len(textareas)}å€‹")
    
    for i, textarea in enumerate(textareas, 1):
        name = textarea.get('name', 'ãªã—')
        placeholder = textarea.get('placeholder', 'ãªã—')
        rows = textarea.get('rows', 'ãªã—')
        cols = textarea.get('cols', 'ãªã—')
        content = textarea.get_text(strip=True)
        
        print(f"\nãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢{i}:")
        print(f"  name: {name}")
        print(f"  placeholder: {placeholder}")
        print(f"  rows: {rows}")
        print(f"  cols: {cols}")
        print(f"  å†…å®¹: {content[:50]}..." if len(content) > 50 else f"  å†…å®¹: {content}")
    print()

def scrape_label_elements(soup):
    """labelè¦ç´ ã¨forå±æ€§ã‚’å–å¾—"""
    print("5. labelè¦ç´ ã¨forå±æ€§")
    print("-" * 30)
    
    labels = soup.find_all('label')
    print(f"labelè¦ç´ æ•°: {len(labels)}å€‹")
    
    for i, label in enumerate(labels, 1):
        for_attr = label.get('for', 'ãªã—')
        text = label.get_text(strip=True)
        
        print(f"ãƒ©ãƒ™ãƒ«{i}:")
        print(f"  for: {for_attr}")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        
        # forå±æ€§ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€å¯¾å¿œã™ã‚‹è¦ç´ ã‚’æ¢ã™
        if for_attr != 'ãªã—':
            target = soup.find(id=for_attr)
            if target:
                print(f"  å¯¾å¿œè¦ç´ : {target.name}ã‚¿ã‚° (id={for_attr})")
            else:
                print(f"  å¯¾å¿œè¦ç´ : è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print()

def scrape_button_elements(soup):
    """buttonè¦ç´ ã‚’å–å¾—"""
    print("6. buttonè¦ç´ ")
    print("-" * 30)
    
    buttons = soup.find_all('button')
    print(f"buttonè¦ç´ æ•°: {len(buttons)}å€‹")
    
    for i, button in enumerate(buttons, 1):
        button_type = button.get('type', 'button')
        name = button.get('name', 'ãªã—')
        value = button.get('value', 'ãªã—')
        text = button.get_text(strip=True)
        
        print(f"ãƒœã‚¿ãƒ³{i}:")
        print(f"  type: {button_type}")
        print(f"  name: {name}")
        print(f"  value: {value}")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        print()

def scrape_form_validation_attributes(soup):
    """ãƒ•ã‚©ãƒ¼ãƒ ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å±æ€§ã‚’å–å¾—"""
    print("7. ãƒ•ã‚©ãƒ¼ãƒ ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å±æ€§")
    print("-" * 30)
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å±æ€§ã‚’æŒã¤è¦ç´ ã‚’æ¤œç´¢
    validation_attrs = ['required', 'pattern', 'min', 'max', 'minlength', 'maxlength']
    
    for attr in validation_attrs:
        elements = soup.find_all(attrs={attr: True})
        if elements:
            print(f"\n{attr}å±æ€§ã‚’æŒã¤è¦ç´ : {len(elements)}å€‹")
            for i, elem in enumerate(elements, 1):
                tag = elem.name
                name = elem.get('name', 'ãªã—')
                attr_value = elem.get(attr, 'ãªã—')
                
                print(f"  {i}. {tag}ã‚¿ã‚° (name: {name})")
                print(f"     {attr}: {attr_value}")
    print()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ãƒ¡ãƒ‹ãƒ¥ãƒ¼å½¢å¼ã§å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠ"""
    menu = [
        ("ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®åŸºæœ¬æƒ…å ±", scrape_form_basic_info),
        ("inputè¦ç´ ã®è©³ç´°æƒ…å ±", scrape_input_elements),
        ("selectè¦ç´ ã¨optionè¦ç´ ", scrape_select_elements),
        ("textareaè¦ç´ ", scrape_textarea_elements),
        ("labelè¦ç´ ã¨forå±æ€§", scrape_label_elements),
        ("buttonè¦ç´ ", scrape_button_elements),
        ("ãƒ•ã‚©ãƒ¼ãƒ ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å±æ€§", scrape_form_validation_attributes),
    ]
    
    print("ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç·´ç¿’")
    print("å¯¾è±¡: ãƒ•ã‚©ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ (/form)")
    print("=" * 50)
    
    for i, (name, _) in enumerate(menu, 1):
        print(f"{i}. {name}")
    print("0. çµ‚äº†")
    print("=" * 50)
    
    # ä¸€åº¦ã ã‘Webãƒšãƒ¼ã‚¸ã‚’å–å¾—
    soup = get_soup()
    
    while True:
        try:
            choice = int(input("\nå®Ÿè¡Œã—ãŸã„ç•ªå·ã‚’é¸ã‚“ã§ãã ã•ã„: "))
        except ValueError:
            print("âŒ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            continue
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue
        
        if choice == 0:
            print("ğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        elif 1 <= choice <= len(menu):
            print("\n" + "=" * 50)
            menu[choice - 1][1](soup)
        else:
            print("âŒ æœ‰åŠ¹ãªç•ªå·ã‚’é¸ã‚“ã§ãã ã•ã„")

if __name__ == "__main__":
    main()
